# 数据挖掘导论
### Pang-Ning Tan, Michael Steinbach
---

# 第1章 绪论
- 数据挖掘是一种将传统的数据分析方法与处理大量数据的复杂算法相结合的技术
- 数据挖掘探查和分析新的数据类型，用新方法分析旧有数据类型
- 数据挖掘是在大型数据存储库中，自动的发现有用信息的过程
- 信息检索(information retrieval)不属于数据挖掘
- 数据库中知识发现(knowledge discovery in database, KDD)
    - 将未加工的数据转换为有用信息的整个过程
    - 步骤
        - 数据预处理：特征选择，维归约，规范化，选择数据子集
        - 数据挖掘
        - 后处理：模式过滤，可视化，模式表示
    - 预处理(preprocessing)
        - 将未加工的输入数据转换成适合分析的形式
        - 融合多个数据源的数据，清洗数据，消除噪声和重复的观测值，选择与当前数据挖掘任务相关的记录和特征
    - 结束循环(closing the loop)：将数据挖掘结果集成到决策支持系统的过程
    - 后处理(postprocessing)
        - 确保只将有效和有用的结果集成到决策支持系统
        - 可视化，从各种不同视角探查数据和数据挖掘结果
        - 使用统计度量或假设检验，删除虚假的数据挖掘结果
- 数据挖掘要解决的问题
    - 可伸缩：搜索策略，数据结构，非内存算法，抽样技术，并行和分布算法
    - 高维性
    - 异种数据和复杂数据
    - 数据的所有权与分布
    - 非传统的分析：传统的统计方法基于假设-检验模式
- 数据挖掘与其他领域
    - 统计学：抽样，估计，假设检验
    - 人工智能、模式识别和机器学习的搜索算法、建模技术和学习理论
    - 最优化、进化计算、信息论、信号处理、可视化、信息检索
    - 数据库系统的有效存储、索引、查询处理
    - 高性能（并行）计算技术和分布式技术处理海量数据集

## 数据挖掘任务
- 数据挖掘任务
    - 预测任务
        - 根据其他属性的值，预测特定属性的值
        - 目标变量(target variable)/因变量(dependent variable)
        - 说明变量(explanatory variable)/自变量(independent variable)
    - 描述任务
        - 导出概括数据中潜在联系的模式（相关、趋势、聚类、轨迹、异常）
        - 描述性数据挖掘任务通常是探查性的，常常需要后处理技术验证和解释结果
- 4种主要的数据挖掘任务
    - 预测建模(predictive modeling): 分类(classification), 回归(regression)
    - 关联分析(assocaition analysis)：发现描述数据中的强关联特征
    - 聚类分析(cluster analysis)：发现紧密相关的观测值组群，使得与属于不同簇的观测值相比，属于同一簇的观测值相互之间尽可能相似
    - 异常检测(naomaly detection): 识别特征显著不同的观测值，即异常点(anomaly)/离群点(outlier)

# 第2章 数据
- 数据类型：定量、定性、序列、关联……
- 数据质量：噪声、离群点、遗漏、不一致、重复、偏差、样本不能代表总体……
- 使数据适合挖掘的预处理：提高数据质量，适应特定数据挖掘技术或工具，连续属性离散化，减少数据集属性数目……
- 根据数据联系分析数据
    - 一种数据分析的方法：找出数据对象之间的联系，使用联系而不是数据对象本身分析数据
    - 计算数据对象之间的相似度或距离

## 数据类型
- 数据集：数据对象的集合
- 数据：记录，点，向量，模式，事件，案例，样本，观测，实体
- 数据对象用一组刻画对象基本特性的属性描述
- 属性：变量，特性，字段，特征，维

### 属性与度量
- 属性(attribute): 对象的性质或特性
    - 属性不是数字或符号
    - 为了讨论和精确分析对象的特性，通过测量标度为属性赋予数字或符号
- 测量标度(measurement scale): 将数值或符号值与对象的属性相关联的规则/函数
    - 测量过程是使用测量标度将一个值与一个特定对象的特定属性相关联
- 属性类型
    - 属性的性质不必与度量它的值的性质相同
    - 属性类型有助于理解数据的意义
    - 通常将属性的类型称为测量标度的类型
- 数值的性质：相异性，序，四则运算
- 4种类型的属性
    - 标称(nominal)
    - 序数(ordinal)
    - 区间(interval)
    - 比率(ratio)
- 分类/定性属性：catagorial/qualitative, 标称和序数，即使使用数表示，也应当视为符号
- 定量/数值属性：quantitative/numeric, 区间和比率，用数表示，整数值或连续值
- 不同类型的属性允许不同的变换
- 非对称属性(asymmetric attribute)

### 数据集的类型
- 3种数据集
    - 记录数据
    - 基于图形的数据
    - 有序的数据
- 数据集的特性
    - 维度(dimensionality)
        - 数据集的维度是数据集中对象的属性数目
        - 分析高位数据有时会陷入维度灾难(curse of dimensionality)
        - 数据预处理的一个目的是减小维度，即维归约(dimensionality reduction)
    - 稀疏性(sparsity)
        - 只有非0值才需要存储和处理，节省大量计算时间和存储空间
        - 有些数据挖掘算法仅适合处理稀疏数据
    - 分辨率(resolution)
        - 通常可以在不同分辨率下得到数据，不同分辨率下数据的性质不同
        - 数据的模式依赖于分辨率
- 记录数据
    - 事务数据(transaction data)
    - 数据矩阵：数据集中的所有数据对象都具有相同的数值属性集
    - 稀疏数据矩阵
- 基于图形的数据
    - 带有对象之间联系的数据：对象之间存在链接
    - 具有图形对象的数据：对象具有结构，对象包含具有联系的子对象，子结构挖掘
- 有序数据
    - 属性涉及时间或空间的联系
    - 时序数据(sequential data)：记录数据的扩充，每个记录包含时间
    - 序列数据(sequence data)：有序序列要考虑项的位置
    - 时间序列数据(time series data)：一种特殊的时序数据，每个记录都是一个时间序列(time series)，即一段时间的测量序列；时间自相光(temporal autocorrelation)
    - 空间数据：空间自相关(spatial autocorrelation)，物理上靠近的对象趋向于其他方面相似

## 数据质量
- 数据挖掘通常不能在数据源头控制质量
- 数据挖掘控制数据质量的两个方面
    - 数据质量问题的检测和纠正：数据清洗(data cleaning)
    - 使用可以容忍低质量数据的算法

### 测量和数据收集问题
- 测量误差(measurement error)
- 数据收集错误(data collection error)
- 涉及测量误差的问题：噪声、伪像、偏倚、精度、准确度
- 可能同时导致测量和数据收集的数据质量问题：离群点、遗漏、不一致的值、重复数据
- 噪声和伪像
    - 噪声是测量误差的随机部分，通常用于包含时间或空间分量的数据
    - 常常可用信号或图像处理技术降低噪声，但难以完全清除噪声
    - 鲁棒算法(robust algorithm)：在噪声干扰下也能产生可接受的结果
    - 伪像(artifact)：数据的确定性失真
- 精度、偏倚和准确率
    - 精度(precision)：同一个量的重复测量值的接近程度
    - 偏倚(bias)：测量值与被测量之间的系统的变差
    - 准确率(accuracy)：被测量的测量值与实际值的接近程度
    - 精度通常用值集合的标准差度量
    - 偏倚通常用值集合的均值与通过外部手段测出的已知值之间的差度量
    - 准确率依赖于精度和偏倚，有效数字(significant digit)
- 离群点(outlier)
    - 离群点是具有不同于数据集中其他大部分数据对象的特征的数据对象
    - 异常(anomalous)对象，异常值
    - 区别离群点和噪声：离群点可以是合法的数据对象或值，异常检测的目标是离群点
- 遗漏值
    - 一个对象遗漏一个或多个属性值
    - 信息收集不全，或某些属性不能用于所有对象
    - 处理遗漏值的策略
        - 删除数据对象或属性
        - 估计遗漏值
        - 在分析时忽略遗漏值
- 不一致的值
    - 检测和纠正错误
    - 查阅外部信息源
    - 纠正不一致需要额外的或冗余的信息
- 重复数据
    - 数据集可能包含重复或几乎重复的数据对象
    - 数据集是否允许重复数据
    - 去重复(deduplication)

### 关于应用的问题
- 从应用角度考虑数据质量问题：数据时高质量的，如果它适合预期的应用
- 时效性
- 相关性
    - 可用的数据必须包含应用需要的信息
    - 抽样偏倚(sampling bias)：样本包含的不同类型的对象与它们在总体中的出现情况不成比例
- 关于数据的知识
    - 理想情况下，数据集附有描述数据的文档
    - 文档质量的好坏决定它是支持还是干扰其后的分析


## 数据预处理
- 重要的数据预处理思想和方法
    - 聚集
    - 抽样
    - 维归约
    - 特征子集选择
    - 特征创建
    - 离散化和二值化
    - 变量变换

### 聚集(aggregation)
- 将两个或多个对象合并为单个对象
- 聚集的目的
    - 数据归约导致的较小数据集需要较少的内存和处理时间，可以使用开销更大的数据挖掘算法
    - 通过高层而不是低层数据视图，聚集有范围或标度转换的作用
    - 对象或属性群的行为通常比单个对象或属性的行为更加稳定
        - 统计学事实：相对于被聚集的单个个体，平均值、总数等聚集量具有较小的变异性
- 聚集的缺点：丢失细节

### 抽样
- 抽样是一种选择数据对象子集进行分析的常用方法
- 抽样可以压缩数据量，使用更好但开销更大的数据挖掘算法
- 有效抽样的主要原理：如果样本有代表性，则使用样本与使用整个数据集几乎一样
- 最好的抽样方案：确保以很高的概率得到具有代表性的样本，选择合适的样本容量和抽样技术
- 抽样方法
    - 简单随机抽样(simple random sampling)：等概率抽样
    - 分层抽样(stratified sampling)：从预先指定的组抽样
    - 无放回抽样
    - 有放回抽样
- 选择合适的样本容量
    - 较大的样本容量增大样本具有代表性的概率，但抵消抽样带来的许多好处
    - 较小的样本容量可能丢失模式，或检测出错误的模式
- 渐进抽样/自适应抽样(progressive/adaptive sampling)
    - 从小样本开始，增加样本容量直至得到足够容量的样本
    - 不需要在开始确定样本容量
    - 需要评估样本容量的方法，确定样本是否足够大
    - 预测模型的准确率随样本容量增加，在某一点准确率的增加区域稳定

### 维归约
- 维归约的优点
    - 若维度较低，许多数据挖掘算法的效果更好
        - 维归约可以删除不相关的特征并降低噪声
        - 维灾难
    - 维归约使模型更容易理解，模型只涉及较少属性
    - 容易数据可视化，观察属性对或三元组属性，组合的数目大大减少
    - 降低数据挖掘算法的时间和内存需求
- 维灾难
    - 随着数据维度增加，许多数据分析变得非常困难
    - 随着数据维度增加，数据在它占据的空间中越来越稀疏
        - 分类：没有足够数据对象创建模型，所有对象被指派到一个类
        - 聚类：点之间的密度和距离的定义失去意义
- 维归约的线性代数技术
    - 使用线性代数技术，将数据从高维空间投影到低维空间
    - 主成分分析(PCA, Principal Components Analysis)：找出新属性（主成分），新属性是原属性的线性组合，相互正交，捕获数据的最大变差
    - 奇异值分解(SVD, Singular Value Decomposition)

### 特征子集选择
- 降低维度的另一个方法是仅使用特征的一个子集
- 冗余特征：重复了包含在一个或多个其他属性中的许多或所有信息
- 不相关特征：包含对数据挖掘任务几乎完全无用的信息
- 特征子集的选择方法
    - 理想方法：将所有可能的特征子集作为感兴趣的数据挖掘算法的输入，选取结果最好的子集；大部分情况行不通
    - 嵌入方法(embedded approach)：特征选择是数据挖掘算法的一部分，数据挖掘算法运行期间由算法本身决定使用或忽略哪些属性
    - 过滤方法(filter approach)：使用某种独立于数据挖掘任务的方法，在数据挖掘算法前进行特征选择
    - 包装方法(wrapper approach)：将目标数据挖掘算法作为黑盒，类似理想方法，但通常不枚举所有可能的子集
- 特征子集选择体系结构
    - 子集评估度量
    - 控制新的特征子集产生的搜索策略
    - 停止搜索判断
        - 迭代次数
        - 子集评估度量值是否最优或超过给定阈值
        - 一个特定大小的子集是否已经得到
        - 使用搜索策略得到的选择是否可以实现改进
    - 验证过程
        - 特征子集产生的结果是否比使用所有特征产生的结果好
        - 使用不同的特征子集，比较数据挖掘算法在不同子集上的运行结果
- 特征加权：特征越重要，权值越大

### 特征创建
- 由旧属性集创建新属性集，更有效地捕获数据集中的重要信息
- 特征提取(feather extraction)
    - 由原始数据创建新的特征集
    - 最常使用的特征提取技术都是高度针对具体领域的
    - 新领域的数据挖掘的一个关键任务是开发新的特征和特征提取方法
- 映射数据到新的空间
    - 使用一种完全不同的视角挖掘数据可能揭示重要和有趣的特征
    - 傅里叶变换(Fourier transform)
    - 小波变换(wavelet transform)
- 特征构造
    - 原始数据集的特征具有必要的信息，但其形式不适合数据挖掘算法
    - 从原特征构造一个或多个新特征

### 离散化(discretization)和二元化(binarization)
- 连续属性离散化
    - 两个子问题
        - 确定类的数目
        - 确定分割点(split point)
    - 非监督离散化(unsupervised): 不使用类信息
        - 等宽(equal width)
        - 等频率(equal frequency)
        - 等深(equal depth)
    - 监督离散化(supervised)：使用附加的类信息/类标号
        - 基于熵(entropy)的方法是最有前途的离散化方法之一
        - 区间的熵是区间纯度的度量
        - 把每个可能值看作可能的分割点，将区间分为两部分，让两个结果区间产生最小熵；取熵最大的区间，重复分割过程，直至区间的个数到达指定数目或满足终止条件
- 具有过多值的分类属性：减少类的数目

### 变量变换(variable transformation)
- 简单函数
    - 指数、对数、倒数、绝对值、三角函数……
    - 统计学中，变量变换常用于将不具有正态分布的数据变换为具有正态分布的数据
    - 弄清变换的效果
        - 是否需要保序
        - 是否作用于所有的值，特别是0和负值
        - 对0和1之间的值有何特别影响
- 规范化/标准化(normalization/standardization)
    - 统计学中，规范化可能与使变量正态的变换混淆
    - 规范化或标准化的目标是使整个值的集合具有特定的性质
    - 创建具有均值0和标准差1的新变量
        - $x' = (x - \bar{x}) / s_x$
        - 均值和标准差受离群点的影响很大，常用中位数(median)和绝对标准差(absolute standard deviation)替代


## 相似性和相异性的度量
- 将数据变换到相似性/相异性空间：很多情况下，一旦计算出相似性或相异性，就不再需要原始数据
- 相似度(similarity)
- 相异度(dissimilarity)
- 邻近度(proximity)
- 距离(distance)是具有特定性质的相异度


# 第3章 探索数据
- 探测性数据分析(Exploratory Data Analysis, EDA)
- 汇总统计、可视化和联机分析处理(OLAP)



# 第4章 分类：基本概念、决策树与模型评估
# 第5章 分类：其他技术
# 第6章 关联分析：基本概念与算法
# 第7章 关联分析：高级概念
# 第8章 聚类分析：基本概念和算法
# 第9章 聚类分析：其他问题和算法
# 第10章 异常检测