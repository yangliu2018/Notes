# 统计学习方法
##### 李航
---
# 第1章 统计学习方法概论

## 统计学习
- 统计学习( statistical learning): 关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科
- 统计学习 = 统计机器学习 (statistical machine learning)
- 机器学习通常特指统计机器学习
- 统计学习的特点
    - 统计学习以计算机及网络为平台，建立在计算机及网络之上
    - 统计学习以数据为研究对象，是数据驱动的学科
    - 统计学习的目的是对数据进行分析和预测
    - 统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析
    - 统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并在发展中逐步形成独自的理论体系和方法论
- 学习：能够通过执行某个过程改进其性能的系统
- 统计学习：计算机系统通过运用数据及统计方法提高系统性能的机器学习
- 统计学习的对象
    - 从数据出发，提取数据特征，抽象数据模型，发现数据中的知识，对数据分析和预测
    - 基本假设：同类数据具有一定的统计规律性
    - 统计学习中以变量或变量组表示数据
    - 数据分为连续变量和离散变量
- 统计学习的目的
    - 对数据预测和分析，特别是对未知新数据进行预测和分析
    - 对数据的预测可以是计算机智能化，或提升计算机的某些性能
    - 对数据的分析可以让人们获取新的知识，给人们带来新的发现
    - 对数据的预测和分析师通过构建概率统计模型实现的
    - 统计学习的总目标是考虑学习什么模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时考虑尽可能提高学习效率
- 统计学习的方法
    - 统计学习的方法是基于数据构建统计模型从而对数据进行预测与分析
    - 分类
        - 监督学习 (supervised learning)
        - 非监督学习 (unsupervised learning)
        - 半监督学习 (semi-supervised learning)
        - 强化学习 (reinforcement learning)
    - 监督学习的方法
        - 从给定的、有限的、用于学习的训练数据 (training data)集合出发，假设数据是独立同分布产生的
        - 假设要学习的模型属于某个函数的集合，称为假设空间 (hypothesis space)
        - 应用某个评价标准 (evaluation criterion)从假设空间中选取一个最优的模型，使它对已知训练数据及未知测试数据(test data)在给定的评价标准下有最优的预测
        - 最优模型的选择由算法实现
    - 统计学习方法的三要素：模型(model)、策略(strategy)、算法(algorithm)
    - 实现统计学习方法的步骤
        - 得到一个有限的训练数据集合
        - 确定包含所有可能的模型的假设空间，即学习模型的集合
        - 确定模型选择的准则，即学习的策略
        - 实现求解最优模型的算法，即学习的算法
        - 通过学习方法选择最优模型
        - 利用学习的最优模型对新数据进行预测或分析
- 统计学习的研究
    - 统计学习方法(statistical learning method)
        - 开发新的学习方法
    - 统计学习理论(statistical learning theory)
        - 探求统计学习方法的有效性和效率
        - 统计学习的基本理论问题
    - 统计学习应用(application of statistical learning)
        - 将统计学习方法应用到实际问题中去，解决实际问题
- 统计学习的重要性
    - 统计学习的应用：人工智能，模式识别，数据挖掘，自然语言处理，语音识别，图像识别，信息检索，生物信息……
    - 统计学习学科在科学技术中的重要性
        - 统计学习是处理海量数据的有效方法
        - 统计学习是计算机智能化的有效手段
        - 统计学习是计算机科学发展的一个重要组成部分
            - 计算机科学的三维：系统，计算，信息
            - 统计学习属于信息维度，且是信息维度的核心

## 监督学习
- 监督学习的任务：学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测（系统的输入输出与学习的输入输出不同）
### 基本概念
- 输入空间(input space)：所有可能的输入值的集合
- 输出空间(ouput space)：所有可能的输出值的集合
    - 输入/输出是有限的，输入/输出空间可以是无限的
    - 输入空间与输出空间可以是同一个空间，也可以是不同的空间
    - 通常输出空间远小于输入空间
- 输入：实例(instance)，通常由特征向量(feature vector)表示
- 特征空间(feature space)：所有特征向量存在的空间
    - 特征空间的每一维对应一个特征
    - 有时假设输入空间与特征空间是相同空间，不予区分
    - 有时假设输入空间与特征空间是不同空间，将实例从输入空间映射到特征空间
    - 模型实际上是定义在特征空间上的
- 输入与输出是分别定义在输入/特征空间与输出空间上的随机变量的取值，变量记作X, Y, 变量的值记作x, y
- 输入实例x的特征向量$ x = (x^{(1)}, x^{(2)},..., x^{(i)}, ..., x^{(n)})^T $
    - $ x^{(i)} $：输入变量x的第i个特征
    - $ x_i $：一组输入变量中的第i个输入变量
- 训练集 $ T = {(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)} $
    - 样本(sample)/样本点：输入输出对
- 预测问题的分类
    - 回归问题：输入变量与输出变量均为连续变量的预测问题
    - 分类问题：输出变量为有限个值的离散变量的预测问题
    - 标注问题：输入变量与输出变量均为变量序列的预测问题
- 联合概率分布
    - 监督学习关于数据的基本假设：监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P(X, Y)
    - P(X, Y)为分布函数或分布密度函数
    - 训练数据与测试数据被视为按照联合概率分布P(X, Y)独立同分布产生
- 假设空间
    - 监督学习的目的在于学习一个从输入到输出的映射，这一映射由模型表示
    - 模型属于输入空间到输出空间的映射的集合，即假设空间(hypothesis space)
    - 假设空间是学习的范围
    - 监督学习的模型可以是概率模型或非概率模型
        - 条件概率分布P(Y|X)
        - 决策函数(decision function) Y = f(X)

### 问题的形式化
- 监督学习分为学习和预测两个过程，由学习系统和预测系统完成
    - 先用训练数据集学习一个模型
    - 再用模型对测试样本集进行预测

### 统计学习三要素
- 方法 = 模型 + 策略 + 算法
#### 模型
- 统计学习首要考虑的问题是学习什么样的模型
- 监督学习的模型是所要学习的条件概率分布或决策函数
- 模型的假设空间包含所有可能的条件概率分布或决策函数
- 假设空间F
    - 决策函数的集合 F = {f | Y = f(X)}
    - 条件概率的集合 F = {P | P(Y|X)}

#### 策略
- 统计学习接着需要考虑的问题是按照怎样的准则学习或选择最优的模型
- 统计学习的目标在于从假设空间中选取最优模型
- 损失函数：度量模型一次预测的好坏
- 风险函数：度量平均意义下模型预测的好坏
- 损失函数L(Y, f(X))是预测值f(X)和真实值Y的非负实值函数
- 常见的损失函数(loss function)
    - 0-1损失函数(0-1 loss function)
    - 平方损失函数(quadratic loss function)
    - 绝对损失函数(absolute loss function)
    - 对数损失函数(logarithmic loss function)/对数似然损失函数(log-likelihood loss function)
- 风险函数(risk function)/期望损失(expected loss)
- 经验风险(empirical risk)/经验损失(empirical loss)
- 期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本的平均损失
    - 根据大数定律，样本容量趋于无限时，经验风险趋于期望风险
    - 由于训练样本数目有限，用经验风险估计期望风险往往并不理想，要对经验风险进行一定的矫正
- 监督学习的两个基本策略
    - 经验风险最小化
    - 结构风险最小化
- 经验风险最小化(ERM, empirical risk minimization)
    - 经验风险最小的模型是最优模型
    - 实例：极大似然估计(maximum likelihood estimation)：条件概率分布模型，对数损失函数
    - 样本容量很小时，经验风险最小化会产生过拟合现象
- 结构风险最小化(SRM, structural risk minimization)
    - 结构风险最小的模型是最优模型
    - 结构风险最小化等价于正则化(regularization)
    - 结构风险 = 经验风险 + 表示模型复杂度的正则化项(regularizer)或罚项(penalty term)
    - 复杂度表示对复杂模型的惩罚
    - 结构风险小要求经验风险和模型复杂度同时小
    - 结构风险小的模型往往对训练数据和未知的测试数据均有较好的预测
    - 实例：最大后验概率估计(maximun posterior probability estimation, MAP)
- 监督学习问题转变为经验或结构风险函数的最优化问题，经验或结构风险函数是最优化的目标函数

#### 算法
- 算法指学习模型的具体计算方法
- 统计学习问题归结为最优化问题，通常没有解析解，需要用数值计算的方法求解
- 如何保证找到全局最优解，并使求解的过程非常高效
- 统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法

## 模型评估与模型选择

### 训练误差与测试误差
- 训练/测试误差是模型关于训练/测试数据集的平均损失
- 误差率(error rate)
- 准确率(accuracy)

### 过拟合与模型选择
- 过拟合：模型过于复杂，对已知数据预测很好，对未知数据预测很差
- 模型选择：避免过拟合并提高模型的预测能力
- 模型复杂度增大时
    - 训练误差逐渐减小并趋向于0
    - 测试误差先减小，达到最小值后又增大
- 常用的模型选择方法：正则化，交叉验证

## 正则化与交叉验证
- 正则化(regularization)
    - 结构风险最小化策略的实现
    - 经验风险 + 正则化项(regularizer)/罚项(penalty term)
    - 正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大
    - 正则化项可以是模型参数向量的范数
    - 从贝叶斯估计得角度，正则化项对应于模型的先验概率
- 奥卡姆剃刀原理：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单的模型才是最好的模型

### 交叉验证
- 样本数据充足，模型选择的简单方法是随机将数据集分成3份
    - 训练集(training set): 训练模型
    - 验证集(validation set): 选择模型
    - 测试集(test set): 评估学习方法
    - 在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型
- 交叉验证：实际应用中数据不充足，重复使用数据，将给定的数据切分为训练集与测试集，反复进行训练、测试和模型选择
- 简单交叉验证
    - 随机地将已给数据分为两部分，分别作为训练集和测试集
    - 用训练集在各种条件下训练模型，得到不同的模型
    - 在测试集上评价各个模型的测试误差，选择测试误差最小的模型
- S折交叉验证(S-fold cross validation)
    - 随机得将已给数据切分为S个互不相交的大小相同的子集
    - 用S-1个子集训练模型，用剩下的1个子集测试模型，对S中选择重复进行
    - 选择S次评测中平均测试误差最小的模型
- 留一交叉验证(leave-one-out cross validation)
    - S折交叉验证的特殊情况，S = N，N是给定数据集的容量
    - 数据缺乏的情况下使用

## 泛化能力 (generalization ability)
- 学习方法的泛化能力：由该方法学习到的模型对未知数据的预测能力
- 用测试误差评价学习方法的泛化能力依赖于测试数据集，评价结果可能不可靠
- 泛化误差 (generalization error): 学习得到的模型对未知数据预测的误差
- 泛化误差上界 (generalization error bound)
    - 样本容量的函数，样本容量增大时，泛化上界趋于0
    - 假设空间容量的函数，假设空间容量越大，模型越难学，泛化误差上界越大


## 生成模型与判别模型
- 模型的一般形式为决策函数Y = f(X)或条件概率分布P(Y|X)
- 监督学习方法的分类
    - 生成方法(generative approach)
    - 判别方法(discriminative approach)

## 分类问题

### 二分类问题的评价指标
- TP: true positive, 将正类预测为正类
- FN: false negative, 将正类预测为负类
- FP: false positive, 将负类预测为正类
- TN: true negative, 将负类预测为负类
- accuracy: 精确率 P = TP / (TP + FP)
- recall: 召回率 R = TP / (TP + FN)
- F1: 精确率和召回率的调和平均
    - 2 / F1 = 1 / P + 1 / R
    - F1 = 2TP / (2TP + FP + FN)

## 标注问题
- 标注(tagging)问题
    - 分类问题的推广
    - 结构预测(structure prediction)问题的简单形式
    - 输入：观测序列
    - 输出：标记序列或状态序列
    - 可能的标记个数是有限的，但标记序列的个数依序列长度指数级增长

## 回归问题
- 预测输入变量和输出变量的关系
- 回归模型是表示从输入变量到输出变量之间映射的函数
- 回归问题的学习等价于函数拟合：选择一条函数曲线，拟合已知数据且预测未知数据
- 回归问题的分类
    - 按照输入变量的个数：一元回归，多元回归
    - 按照输入变量和输出变量的关系：线性回归，非线性回归
- 回归学习最常用的损失函数：平方损失函数
    - 回归问题可以由最小二乘法(least squares)求解

---

# 第2章 感知机
- 感知机(perceptron)
    - 二类分类的线性分类模型
    - 输入：实例的特征向量
    - 输出：实例的类别，取值+1和-1
    - 感知机对应特征空间中将实例划分为正负两类的分离超平面，属于判别模型
    - 目的：求出将训练数据线性划分的超平面
    - 损失函数：误分类
    - 损失函数最小化算法：梯度下降法
    - 感知机学习算法：原始形式、对偶形式
    - 感知机是神经网络和支持向量机的基础

## 感知机模型
- 感知机 f(x) = sign(wx + b)
    - w: 权值(weight), 权值向量(weight vector)，超平面的法向量
    - b: 偏置(bias)，超平面的截距
    - 分离超平面(seperating hyperplane)
    - 符号函数 sign(x) = x / |x|

## 感知机学习策略
- 数据集的线性可分性(linearly separable data set)
- 感知机的学习目标：求一个能将线性可分训练数据集的正实例点和负实例点完全正确分开的分离超平面
- 损失函数
    - 误分类点的总数：不是参数w, b的连续可导函数，不易优化
    - 误分类点到超平面的总距离
- 感知机学习的经验风险函数$L(w,b) = - \sum_{x_i \in M} y_i(w*x_i+b)$
    - 训练集N，超平面S的误分类点集合M
    - y(wx+b): 样本点的函数间隔
- 感知机学习的策略：在假设空间中选取使感知机学习损失函数最小的模型参数w, b，即感知机模型

## 感知机学习算法
- 感知机学习问题转化为求解感知机损失函数最优化问题：随机梯度下降法(stochastic gradient descent)

### 感知机学习算法的原始形式
- 任意选取一个超平面w0, b0，用梯度下降法不断极小化目标函数
- 极小化的过程不是一次是M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降
- 步长，学习率(learning rate): η, [0, 1)
- 感知机学习算法采用不同的初值或选取不同的误分类点时，解可以不同
    - 感知机学习算法存在多个解
    - 感知机学习算法的解依赖于初值的选择和迭代过程中误分类点的选择顺序
    - 为了得到唯一超平面，需要对分离超平面增加约束条件
- $w = w + η y_i x_i, b = b + η y_i$

### 算法的收敛性
- 将偏置b并入权重向量w: $\hat w = (w, b), \hat x = (x, 1) \to \hat w * \hat x = w * x + b$
- 误分类次数有上界，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面
- 训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的
- 训练集线性不可分时，感知机学习算法不收敛，迭代结果震荡

### 感知机学习算法的对偶形式
- 感知机模型 $f(x) = sign(\sum_{j=1}^N α_j y_j x_j * x + b), α = (α_1, α_2, ..., α_N)^T$
- $α_i = α_i + η, b = b + η y_i$
- Gram矩阵 $G = [x_i * x_j]_{N*N}$ 训练点之间的内积


# 第3章 k近临法
- k-nearest neighbor, k-NN
    - 一种基本分类与回归方法
    - 输入：实例的特征向量，特征空间中的点
    - 输出：实例的类别，可以有多个类
    - 对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测
    - 不具有显式的学习过程
    - 模型：利用训练数据集对特征向量空间进行划分
    - k近邻法的3个基本要素：k值的选择，距离度量，分类决策规则
    - k近邻发的1个实现方法：kd树

## k近邻算法
- 根据给定的距离度量，在训练集T中找出与x最临近的k个点，涵盖这k个点的x的邻域记为$N_k(x)$
- 在$N_k(x)$中根据分类决策规则（如多数表决）决定x的类别y: $y = argmax_j \sum_i I(y_i = c_j), i = 1,2,...N; j = 1,2,...,K$
- 最近邻法：k = 1的k近邻法，将训练数据集中与x最邻近点的类作为x的类

## k近邻模型
- k近邻法的模型是特征空间的划分
- k近邻法的模型由3个基本要素组成
### 距离度量
- 一般为欧氏距离，也可以是拉普拉斯距离或曼哈顿距离
- 不同距离度量确定的最近邻点可能是不同的
### k值的选择
- k值的选择对k近邻法的结果产生重大影响
- k值小
    - 近似误差(approximation error)小
    - 估计误差(estimation error)大
    - 预测结果对近邻实例点非常敏感，易受噪声影响而出错
    - 模型复杂，容易发生过拟合
- k值大
    - 用较大的邻域训练实例进行预测
    - 近似误差大，估计误差小
    - 与预测实例较远（不相似）的训练实例会对预测起作用，使预测错误
    - 模型简单，容易发生欠拟合
    - 若k = N，任何输入实例都会被简单预测为训练实例中最多的类，模型过于简单，完全忽略训练实例中的大量有用信息
- 实际应用k一般取一个较小的值，采用交叉验证选取最优的k值
### 分类决策规则
- k近邻法的分类决策规则通常是多数表决，即由输入实例的k个临近的训练实例中的多数类决定输入实例的类
- 多数表决规则(majority voting rule): 损失函数为0-1损失函数时的经验风险最小化

## k近邻法的实现：kd tree
- 对训练数据进行快速k近邻搜索（特征空间维数大，训练数据容量大）
- kd树
    - 一种对k维空间中的实例点进行存储以便快速检索的树形数据结构
    - 二叉树，表示对k维空间的一个划分(partition)
    - 用垂直于坐标轴的超平面将k维空间切分，构成一系列k维超矩形区域
    - 每个结点对应一个k维超矩形区域

# 第4章 朴素贝叶斯法
- 朴素贝叶斯(naive Bayes)法：基于贝叶斯定理与特征条件独立假设的分类方法
    - 对于给定训练数据集，先基于特征条件独立假设学习输入/输出的联合概率分布
    - 然后基于模型，对给定的输入x，利用贝叶斯定理求厚颜概率最大的输出y
    - 朴素贝叶斯法实现简单，学习与预测的效率都很高

## 朴素贝叶斯法的学习与分类
- 朴素贝叶斯法学习先验概率分布P(Y)和条件概率分布P(X|Y)，得到联合概率分布P(X, Y)，求得P(Y|X)
- 条件独立性假设
    - P(x1, x2, ..., xn | yk) = P(x1|yk)P(x2|yk)...P(xn|yk)
    - 分类的特征在类确定的条件下是独立的
    - 条件独立性假设使朴素贝叶斯法简单，但有时会牺牲一定的分类准确率
- 朴素贝叶斯法学习的是生成数据的机制，属于生成模型
- 后验概率最大化等价于期望风险最小化$f(x) = argmax_{c_k}P(Y=c_k|X=x)$
- 朴素贝叶斯法分类器 $y = argmax_{c_k} P(Y=c_k) P(X=x|Y=c_k) = argmax_{c_k}P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)} | Y=c_k)$

## 朴素贝叶斯法的参数估计
### 极大似然估计

### 贝叶斯估计
- 极大似然估计可能出现要估计得概率值为0的情况，影响后验概率的计算结果，使分类产生偏差
- 条件概率的贝叶斯估计
- 先验概率的贝叶斯估计
- 极大似然估计是λ = 0的贝叶斯估计
- 拉普拉斯平滑(Laplace smoothing)是λ = 1的贝叶斯估计
- 若输入变量不是条件独立，存在概率依存关系，朴素贝叶斯法变成贝叶斯网络


# 第5章 决策树
- 决策树(descision tree)是一种基本的分类与回归方法
- 分类问题中的决策树
    - 表示基于特征对实例进行分类的过程
    - if-then规则的集合
    - 定义在特征空间与类空间上的条件概率分布
- 决策树学习的3个步骤
    - 特征选择
    - 决策树的生成
    - 决策树的修剪

## 决策树模型与学习

### 决策树模型
- 分类决策树模型是一种描述对实例进行分类的树形结构
- 决策树
    - 结点(node)
        - 内部节点(internal node): 表示一个特征或属性
        - 叶结点(leaf node): 表示一个类
    - 有向边(directed edge)
- 用决策树分类，从根节点开始，对实例某一项特征进行测试，根据测试结果，将实例分配到其子节点，递归地对实例进行测试并分类，直到到达叶结点，实例分到叶结点的类中

### 决策树学习
- 决策树学习的本质是从训练数据集中归纳一组分类规则
- 与训练数据集不矛盾/能对训练数据正确分类的决策树可能有多个，也可能没有
- 目标：学习一个与训练数据集矛盾较小的决策树，同时具有很好的泛化能力
- 决策树学习是由训练数据集估计条件概率模型
- 决策树学习的损失函数是正则化的极大似然函数
- 决策树学习的策略是以损失函数为目标函数的最小化


# 第6章 逻辑斯谛回归与最大熵模型
- 对数线性模型
    - 逻辑斯谛回归(logistic regression)
    - 最大熵模型(maximum entropy model)

## 逻辑斯谛回归模型
### 逻辑斯谛分布(logsitic distribution)
### 二项逻辑斯谛模型(binomial logistic regression model)
### 多项逻辑斯谛回归(multi-nominal logistic regression model)

## 最大熵模型
- 最大熵原理
    - 学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型
    - 在满足约束条件的模型集合中选择熵最大的模型
- 没有约束时，均匀分布的熵最大
    - 最大熵原理认为模型满足约束条件后，不确定的部分是等可能的
    - 最大熵原理通过熵的最大化表示等可能性

# 第7章 支持向量机
- 支持向量机(support vector machind, SVM)
    - 二类分类模型
    - 模型：定义在特征空间上的间隔最大的线性分类器
    - 核技巧：使SVM成为实质上的非线性分类器
    - 学习策略：间隔最大化
        - 凸二次规划问题(convex quadratic programming)
        - 正则化的合页损失函数最小化问题
    - 学习算法：求解凸二次规划的最优化算法
    - 学习方法
        - 线性可分支持向量机(linear support vector machine in linearly separable case)
            - 训练数据线性可分
            - 硬间隔最大化(hard margin maximization)
            - 硬间隔支持向量机
        - 线性支持向量机(linear support vector machine)
            - 训练数据近似线性可分
            - 软间隔最大化(soft margin maximization)
            - 软间隔支持向量机
        - 非线性支持向量机(non-linear support vector machine)
            - 训练数据线性不可分
            - 核技巧(kernel trick)和软间隔最大化
- 核技巧(kernel trick)
    - 输入空间：欧氏空间或离散集合
    - 特征空间：希尔伯特空间
    - 核函数(kernel function)：将输入空间的输入映射到特征空间的特征向量的内积
    - 使用核函数学习非线性支持向量机，等价于隐式地在高维特征空间中学习线性支持向量机
- 核方法(kernel method)是比支持向量机更一般的机器学习方法

## 线性可分支持向量机与硬间隔最大化
- 感知机与线性可分支持向量机的异同
    - 用分离超平面区分线性可分数据集
    - 感知机：误分类最小策略，无穷多解
    - 线性可分支持向量机：间隔最大化策略，唯一解 
- |wx + b|: 一个点距离分离超平面的远近可以表示分类的确信程度
- y(wx + b): 函数间隔(functional margin)，表示分类预测的正确性和确信度
- 几何间隔(geometric margin)：规范化分离超平面的法向量的模为1的函数间隔，||w|| = 1
- 数据集到超平面的函数/几何距离是所有数据点到超平面的函数/几何距离的最小值
- 支持向量(support vector)
    - 训练数据集的样本点中与分离超平面距离最近的样本点
    - 正例点位于超平面 H1: wx + b = 1
    - 负例点位于超平面 H2: wx + b = -1
    - H1与H2平行，H1与H2之间没有实例点
    - 间隔(margin): H1与H2之间的距离，2 / ||w||
    - 间隔边界：超平面H1和H2，间隔边界外的点不影响分离超平面
    - 支持向量在确定分离超平面中起决定性作用，故称该分类模型为支持向量机
    - 支持向量个数一般很少，支持向量机有较少的重要训练样本确定

### 学习的对偶算法
- 对偶算法(dual algorithm)：应用拉格朗日对偶性，通过求解对偶问题(dual problem)得到原始问题(primal problem)的解

## 线性支持向量机与软间隔最大化
- 训练数据集中有一些特异点(outlier)，将特异点除去后，剩下的大部分样本点是线性可分的
- 对每个样本点引进一个松弛变量

## 非线性支持向量机与核函数
- 求解非线性问题的方法：通过非线性变换将非线性问题变换为线性问题求解
- 在线性支持向量机学习的对偶问题中，目标函数和分类决策函数都只涉及实例与实例的内积，故不需要显式指定非线性变换，而是用核函数替换内积
- 核函数表示通过一个非线性变换后的两个实例的内积

- 常用核函数
    - 多项式核函数(polynomial kernel function)
    - 高斯核函数(gaussian kernel function)
    - 字符串核函数(string kernel function)

## 序列最小最优化算法


# 第8章 提升方法
- 提升方法(boosting method)在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能

## 提升方法AdaBoost算法
- 在PAC学习框架下，强可学习和弱可学习等价
    - 强可学习(strongly learnable): 可学习且正确率很高
    - 弱可学习(weakly learnable): 可学习但正确率仅比随机猜测略好
    - PAC: probably approximately correct, 概率近似正确
    - 发现弱可学习算法比发现强可学习算法容易
    - 将弱可学习算法提升(boost)为强可学习算法
- 提升方法
    - 从弱学习算法出发，反复学习，得到一系列弱分类器(基本分类器)
    - 组合弱分类器，构成一个强分类器
    - 大多数提升方法是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器
- 提升方法的重点
    - 每一轮如何改变训练数据的权值或概率分布
    - 如何将弱分类器组合成一个强分类器
- AdaBoost
    - 提高被前一轮弱分类器错误分类样本的权值，降低被正确分类样本的权值，让没有正确分类的数据被后一轮的弱分类器更大关注
    - 加权多数表决的方法组合弱分类器，分类误差率小的弱分类器的权值高

### AdaBoost算法
1. 初始化训练数据的权值分布
2. 对m = 1, 2, ..., M
    1. 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x)$
    2. 计算$G_m(x)$在训练数据集上的分类误差率$e_m = \sum_{i=1}^N P(G_m(x_i) \neq y_i)$
    3. 计算$G_m(x)$的系数$α_m = \frac 1 2 log \frac {1-e_m} {e_m}$
    4. 更新训练数据集的权值分布
3. 构建基本分类器的线性组合，得到最终分类器

# 第9章 EM算法及其推广
- EM算法(expectation maximization algorithm, 期望极大算法)
    - 迭代算法
    - 用于含隐变量(hidden variable)的概率模型参数的极大似然估计或极大后验概率估计
    - EM算法的每次迭代由2步组成
        - E step: 求期望(expectation)
        - M step: 求极大(maximization)
- 概率模型的变量
    - 观测变量(observable variable)
    - 隐变量(hidden variable)/潜在变量(latent variable)
    - 当概率模型只含有观测变量，可以直接用极大似然估计或贝叶斯估计方法估计模型参数






# 第10章 隐马尔可夫模型
- 隐马尔可夫模型(hidden Markov model, HMM)
    - 可用于标注问题的统计学习模型
    - 描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型
    - 描述由隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程
    - 状态序列(state sequence): 隐藏的马尔可夫链生成的状态的序列
    - 观测序列(observation sequence): 每个状态生成一个观测，产生的观测的随机序列
    - 隐马尔可夫模型由初始概率分布、状态转移概率分布、观测概率分布确定




# 第11章 条件随机场
- 条件随机场(conditional random field, CRF)
    - 给定一组输入随机变量条件下，另一组输入随机变量的条件概率分布模型
    - 假设输出随机变量构成马尔可夫随机场
    